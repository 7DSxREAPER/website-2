<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      RoboComp &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/website/public/css/poole.css">
  <link rel="stylesheet" href="/website/public/css/syntax.css">
  <link rel="stylesheet" href="/website/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/websitepublic/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/websitepublic/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/website">
          RoboComp
        </a>
      </h1>
      <p class="lead">A simple robotics framework.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/website">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/Blog/">Blog</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC15/">GSoC'15</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC16/">GSoC'16</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/projects/">Projects</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/about/">About</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/website/contact/">Contact</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/install/">Install</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
      <a class="sidebar-nav-item" href="http://robocomp.readthedocs.org">Tutorials</a>
      <a class="sidebar-nav-item" href="https://github.com/robocomp">GitHub project</a>
      
      <span class="sidebar-nav-item">Currently v1.0.0</span>
    </nav>

    <p>&copy; 2016. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <i><b>RoboComp</b> is an open-source Robotics framework providing the tools to create and modify software components that communicate through public interfaces. Components may require, subscribe, implement or publish interfaces in a seamless way. Building new components is done using two domain specific languages, IDSL and CDSL. With IDSL you define an interface and with CDSL you specify how the component will communicate with the world. With this information, a code generator creates C++ and/or Python sources, based on CMake, that compile and execute flawlessly. When some of these features have to be changed, the component can be easily regenerated and all the user specific code is preserved thanks to a simple inheritance mechanism.</i>


<hr>

<div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/16/mercedes5/">
        <i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#5</p> System Review
      </a>
    </h1>

    <span class="post-date">16 Aug 2015</span>

    <p><strong>Full object manipulation system</strong> : In this post we will describe the full system developed for manipulating objects using a robotic arm. We start at the highest level, VisualIK (the correction system), and we will go down to the base of the system, the IK (which calculates the final angle of the joints of the robotic arm).</p>

<p><img src="https://github.com/mercedes92/VisualIKExperiment/blob/master/images/Dibujo%20sin%20t%C3%ADtulo.png?raw=true" alt="Alt text"></p>

<p>All the components that will be described below implement the <code>InverseKinematics</code> interface. This interface is defined by a series of data structures:</p>

<ol>
<li><code>Pose6D</code>: It represents a pose with three translations and three rotations [x, y, z, rx, ry, rz]</li>
<li><code>WeightVector</code>: It represents the weight vector of each element of Pose6D. This will help us later in the Levenberg-Marquardt algorithm.</li>
<li><code>TargetState</code>: It is the state when the targets reach the end of their execution by ik. It has some information like the elapsed time, the final error in translations and rotations and the values of each joint.</li>
<li><code>Axis</code>: This structure represents the Cartesian axes. It is necessary for certain special types of targets.</li>
<li><code>Motor</code>: This structure stores the name and the angular value of a motor of the kinematic chain.</li>
</ol>

<p>Also, the interface defines a number of methods:</p>

<ol>
<li><code>getTargetState</code>: this method returns the state of one target, given the name of the robot part that executed the target and the numerical identifier of the target.</li>
<li><code>getPartState</code>: this method returns the state (if the part has pending targets or not) of one of the robot part (RIGHTARM, LEFTARM or HEAD).</li>
<li><code>setTargetPose6D</code>: This method is used to send targets to the ik. We need to indicate the part of the robot that will execute the target, the pose of the target and the weight vector of the pose. It returns the identifier of the target.</li>
<li><code>setTargetAlignaxis</code>: This method sends a special target for the IK. This target is achieved by aligning the end effector to the target axes. We must indicate the the part of the robot that will execute the target, the pose and the axes. It returns the identifier of the target.</li>
<li><code>setTargetAdvanceAxis</code>: This method sends another special target for the IK. The goal is that the end effector advance along a vector in the Cartesian system. We must indicate the part of the robot that will execute the target, the axes of the vector and the dist to advance. Like always, it return the identifier of the target.</li>
<li><code>goHome</code>: this method send a robot part to the idle position.</li>
<li><code>stop</code>: this method abort the execution of the targets of one of the robot part.</li>
<li><code>setJoint</code>: this method changes the angular value of one of the joint of the kinematic chain.</li>
<li><code>setFingers</code>: this method opens and closes the fingers of the end effector.</li>
</ol>

<p>Now, we will describe the basic operation of our components.</p>

<h3>The <code>visualIK</code> component</h3>

<p>The old <a href="http://robocomp.github.io/website/2015/06/17/mercedes3.html"><code>VisualBIK</code> component</a> has been reorganized, resulting in the current <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/visualik"><code>visualik</code> component</a>. The basic principle of operation of the component remains, adding some improvements and changing its communication with the <code>inversekinematics</code> component, by the <code>ikGraphGenerator</code> component.</p>

<p>Its goal remains the same, correct errors produced by the inaccuracies of the joints in the IK. To this end, it bases its operation on a state machine:</p>

<ol>
<li><code>IDLE</code>: It is the resting state of <code>visualik</code>. The component is waiting to receive a target through one of the methods of its interface. When a target comes through the interface (stored in the <code>currentTarget</code> variable), the state machine switches to INIT_BIK and prepares the global variables for the execution of correction.</li>
<li><code>INIT_BIK</code>: in this state, the visualik applies an initial correction to the target (Ec). This correction is based on experience in the correction of previous targets, so the first correction, having no previous experience, will be zero. Then, it sends the target to his proxy of kinematic, the <code>ikGraphGenerator</code> component. Finally, the state of the machine is changed to <code>WAIT_BIK</code>.</li>
<li><code>WAIT_BIK</code>: in this state, the <code>visualik</code> waits the end of execution of the target by the <code>ikGraphGenerator</code>. When the <code>ikGraphGenerator</code> finishes executing the target, the <code>visualik</code> changes his state to <code>CORRECT_ROTATION</code>.</li>
<li><code>CORRECT_ROTATION</code>: It is the latest machine status. In this state is when the <code>visualik</code> does all the calculations in order to correct the deviations and errors of the joints. The procedure is simple: a) by apriltags, the <code>visualik</code> calculates the visual pose of the end effector; b) then, it computes the error vector between the visual pose and the target pose (Ev); c) with this error vector Ev, the <code>visualik</code> corrects the target pose and sends the new position to the <code>ikGraphGenerator</code> component; d) this process is repeated until the error achieved in translation and rotation is less than a predetermined threshold; e) finally the <code>visualik</code> calculates the error vector between the original target and the last corrected target (Ec), with this error the component realizes the first correction in <code>INIT_BIK</code>.</li>
</ol>

<p>Here there is a scheme of the procedure performed by the <code>visualik</code>. It is very summarized to facilitate the understanding of the procedure by the reader:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">1: procedure VISUAL CALIBRATION
2:    Xt = TargetArrives()
3:    Ct = Xt + Ec
4:    sendTargetToGIK(Ct)
5:    Xv = getAprilTagPose(robot)
6:    while (Ev = Xv- Xt) &gt; threshold ^¬ timeOut() do
7:         Xi = getInternalPose(robot)
8:         Xc = Xi + Ev
9:         sendTargetToGIK(Xc)
10:        Xv = getAprilTagPose(robot)
11:   end while
12:   Ec = Xc - Xt
13: end procedure
</code></pre></div>
<p>Like all components developed by robocomp, the <code>visualik</code> needs a configuration file in which the components required (the <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator"><code>ikGraphGenerator</code></a>) and the components to subscribe (the <a href="https://github.com/robocomp/robocomp-robolab/tree/master/components/apriltagsComp"><code>apriltagsComp</code></a>) and other configuration parameters are determined. In this case, a configuration file may have the following elements:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">CommonBehavior.Endpoints=tcp -p 14537
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10242

# Endpoints for interfaces to subscribe:
AprilTagsTopic.Endpoints=tcp -p 12938

# Proxies for required interfaces
InverseKinematicsProxy =inversekinematics:tcp -h localhost -p 10241 # `ikGraphGenerator`

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus-rockin/etc/ficheros_Test_VisualBIK/ursus_bik.xml # the internal model of the robot environment 

# This property is used by the clients to connect to IceStorm.
TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
</code></pre></div>
<h3>The <code>ikGraphGenerator</code> component</h3>

<p>As we explain in the previous <a href="http://robocomp.github.io/website/2015/08/13/mercedes4.html">post</a>, the <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator"><code>ikGraphGenerator</code>component</a> creates and stores a graph representing the work 3D space of the arm, where each node stores the euclidean space pose of the end effector and the configuration of the joints that compose the arm. So the <code>ikGraphGenerator</code> waits until the <code>visualik</code> send a target to him through the <code>InverseKinematics</code> interface. In this moment, the <code>ikGraphGenerator</code> performs four steps:</p>

<ol>
<li>the component searches the node A whose pose is closest to the initial end effector pose and moves the arm there.</li>
<li>the component finds in the graph  the node B whose pose is closest to the target position, disabling those nodes which would make the robot&#39;s arm collide with external objects.</li>
<li>the component computes the shortest path between the node A and the node B and moves the end effector among the nodes to reach the node B.</li>
<li>Finally, the component moves from the graph to the actual target sending the original target to the <code>inversekinematic</code>component and taking the final values of the joints.</li>
</ol>

<p>The <code>ikGraphGenerator</code>needs a config file too:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">CommonBehavior.Endpoints=tcp -p 14536
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10241

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml # the internal model of the robot environment 

# Proxies for required interfaces
InverseKinematicsProxy = inversekinematics:tcp -h localhost -p 10240
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

Ice.Warn.Connections=0
Ice.Trace.Network=0 
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
</code></pre></div>
<h3>The <code>inversekinematic</code> component</h3>

<p>Finally, we will explain the basic component of the system, the <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/inversekinematics"><code>inversekinematic</code> component</a>. As we said in the <a href="http://robocomp.github.io/website/2015/06/15/mercedes2.html">second post</a>, this component receives three types of targets through the <code>InverseKinematics</code> interface:</p>

<ol>
<li><code>POSE6D</code>: the typical target with translations and rotations [tx, ty, tz, rx,ry, rz]. The end effector has to be positioned at coordinates (tx, ty, tz) of the target and align their rotation axes with the target, specified in (rx, ry, rz). This target arrives from the method <code>setTargetPose6D</code>.</li>
<li><code>ADVANCEAXIS</code>: its goal is to move the end effector of the robot along a vector. This target arrives from the method <code>setTargetAdvanceAxis</code>.</li>
<li><code>ALIGNAXIS</code>: Its goal is that the end effector has the axes aligned with the target. This target arrives from the method <code>setTargetAlignaxis</code>.</li>
</ol>

<p>Targets received are stored into the queues of the corresponding robot part and they are executed by order of arrival. When the ik ends the execution of one target, it stores the target into the solved targets queue:</p>

<p><img src="https://github.com/mercedes92/VisualIKExperiment/blob/master/images/iksystem.png?raw=true" alt="Alt text"></p>

<p>The config file of the <code>inversekinematic</code>component is the next:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">CommonBehavior.Endpoints=tcp -p 12207
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10240

# Kinematic chain lists: they stores the joints names.
RIGHTARM=rightShoulder1;rightShoulder2;rightShoulder3;rightElbow;rightForeArm;rightWrist1;rightWrist2
RIGHTTIP=grabPositionHandR # end effector of the RIGHTARM

LEFTARM=leftShoulder1;leftShoulder2;leftShoulder3;leftElbow;leftForeArm;leftWrist1;leftWrist2
LEFTTIP=grabPositionHandL

HEAD=head_yaw_joint;head_pitch_joint
HEADTIP=rgbd_transform

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml #Internal model of the robot environment

 # Proxies for required interfaces
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
</code></pre></div>
<p>When <code>visualik</code> and <code>ikGraphGenerator</code> components submit their targets to the immediately below component, they store the identifier of the target and are waiting until the target be resolved, calling the method <code>getTargetState</code>. So, when the <code>inversekinematic</code> component ends one target execution, the <code>ikGraphGenerator</code> moves the arm with the values given by the <code>inversekinematic</code> component and then, the <code>visualik</code> continues with the corrections.</p>

<p>Having explained the handling system, the next post will explain the planning system developed by ROBOLAB to plan the robot&#39;s actions.</p>

<p>Bye!</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/13/mercedes4/">
        <i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#4</p>, Inverse kinematics Graph Generator
      </a>
    </h1>

    <span class="post-date">13 Aug 2015</span>

    <p><strong>ikGraphGenerator, an alternative to ik</strong> : As the project has progressed, many improvements have emerged. One of them is the new component, <code>ikGraphGenerator</code>. This component has been developed by Professor Luis Manso, and its goal is to remove weight to the inverse kinematics in the process of handling objects with a robotic effector.</p>

<p>One of the problems of the inverse kinematics is that, given a target for a particular end-effector, it can calculate different solutions or values for each motor of the kinematic chain. For example, to pick up a cup, the IK can position the end effector at the target extending his arm more or less forcing or not the elbow or or separating more or less the shoulder. It doesn&#39;t matter that the final position of the chain be more forced or more natural, the goal is reached and the solution is accepted.</p>

<p>This is not convenient, since there is no way to control the trajectory of the arm. The control on the trajectory arm is crucial at certain times, for example when the robot avoids collisions between his arm and another objects (tables, chairs, walls, including his own body). The current <code>inversekinematics</code> component does not allow us this control, so that we need the <code>ikGraphGenerator</code> component.</p>

<p>The main concept is to create a spatial graph where each node stores the pose of the point, [tx, ty, tz, rx, ry, rz], and the set of angular values for each motor of the kinematic chain (that has 7 DOFs). The links connecting the nodes whose positions are close to each other. In this way we can calculate paths between two different and separate points.</p>

<p><img src="https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/ikg.jpg?raw=true" alt="Alt text"></p>

<h3>What does the <code>ikGraphGenerator</code> component?</h3>

<p>Basically the <code>ikGraphGenerator</code> realizes two functions:</p>

<ol>
<li>It calculates the graph with random poses and their respective angular values. To do this, first it defines a spatial cube that represents the workspace of the robot arm. In this space a set of poses are selected and are sent to the <code>inversekinematic</code> component as targets. The poses that are not achievable by the ik are automatically deleted. The resultant graph is stored in a file in order to use it in later calculations.</li>
<li>Once the graph has been calculated and stored, we can send a target to the <code>ikGraphGenerator</code>. First the component searches the node <code>A</code> whose pose is closest to the initial end effector pose and the node <code>B</code> whose pose is closest to the target position (to do this, the <code>ikGraphGenerator</code> uses a fast low-dimension k-d tree). Then, the component calculates a path between node A and node B through the graph  using Dijkstra&#39;s algorithm, so that the arm moves through the graph from the position marked by node A to the position marked by the node B. Finally, in order to achieve the final target, the <code>inversekinematic</code> component is called to compute the final values of the joints, starting from the position marked by the node B.</li>
</ol>

<p><img src="https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/GIK.png?raw=true" alt="Alt text"></p>

<p>In the next post I will describe how the whole system works with all the components, the <code>inversekinematic</code>, the <code>ikGraphGenerator</code> and the <code>visualik</code> component.</p>

<p>Bye!</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/08/nithin9/">
        Till now ... after midterm
      </a>
    </h1>

    <span class="post-date">08 Aug 2015</span>

    <p>Hi all , In this post i will talk about what i have been working on after midterm evaluation. I have spend my time working mostly on packaging supporting libraries for Robocomp.This includes FCL and libccd. FCL is a library for performing three types of proximity queries on a pair of geometric models composed of triangles. libccd is a library for collision detection between two convex shapes.Technically Robocomp is only using FCL but libccd is an dependency of fcl, as i couldn&#39;t find an updated ppa for it i decided to package it too. you can see those packages <a href="https://launchpad.net/%7Eimnmfotmal">here</a>  </p>

<p>Also i added ability for generating robocomp source packages for different distributions. Initially i added the option only for trusty, now we could generate packages for any distribution.I worked a bit on build tools also. Currently if someone created an workspace and made a github repo of it, some one else cant use it as we store the repo names in ~/.config directory, so i added an option to reinit an repo.  </p>

<p>well i guess thats all for now  </p>

<hr>

<p>Nithin Murali</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/07/25/nithin10/">
        Setting up ppa
      </a>
    </h1>

    <span class="post-date">25 Jul 2015</span>

    <h2>Setting up an ppa in launchpad</h2>

<p>After creating an launchpad account First you need to create and publish an OPENPGP key</p>

<h3>Generating your key in Ubuntu</h3>

<p>The easiest way to generate a new OpenPGP key in Ubuntu is to use the Passwords and Encryption Keys tool. </p>

<p><strong>Step 1</strong> open Passwords and Encryption Keys.</p>

<p><strong>Step 2</strong> Select File &gt; New, select PGP Key and then follow the on-screen instructions.</p>

<p>Now you&#39;ll see your new key listed in the Passwords and Encryption Keys tool. (it may take some time)</p>

<h3>Publishing your key</h3>

<p>Your key is useful only if other people can verify items that you sign. By publishing your key to a keyserver, which acts as a directory of people&#39;s public keys, you can make your public key available to anyone else.Before you add your key to Launchpad, you need to push it to the Ubuntu keyserver.  </p>

<p><strong>Step 1</strong> Open Passwords and Encryption Keys.</p>

<p><strong>Step 2</strong> Select the My Personal Keys tab, select your key.</p>

<p><strong>Step 3</strong>  Select Remote &gt; Sync and Publish Keys from the menu. Choose the Sync button. (You may need to add htp://keyserver.ubuntu.com to your key servers if you are not using Ubuntu.)</p>

<p>It can take up to thirty minutes before your key is available to Launchpad. After that time, you&#39;re ready to import your new key into Launchpad!  </p>

<p>OR you can direclty to go <code>http://keyserver.ubuntu.com/</code> on your browser and add the PGP key there</p>

<h3>Register your key in launchpad</h3>

<p>fire up an terminal and run <code>gpg --fingerprint</code> should give you fingerprints of all the keys. copy paste the required fingerprint into launchpad</p>

<h3>Sign Ubunutu Code of Conduct</h3>

<p>Download the ubuntu code of conduct form launchpad
<code>gpg --clearsign UbuntuCodeofConductFile</code>  will sign the file
now copy the contents of the signed file and paste in launchpad</p>

<h3>Wrapping Up</h3>

<p>Now everything is set up. make sure you have some key in <code>OPENPGP Keys</code> section and also the signed code of code of conduct as <code>Yes</code> as shown.
<img src="./launchpad.png" alt=""></p>

<h2>Uploading package to ppa</h2>

<p>launchpad will only accept source packages and not binary.Launchpad will then build the packages. For building source packages we are using debuild which is a wrapper around the <em>dpkg-buildpackage + lintian</em>. so you will need to install debuild and dput on your system;</p>

<p>The source_package.cmake script is used to create debian source package.</p>

<p>The main CMakeLists.txt file defines a target <code>spackage</code> that builds the source package in build/Debian with <code>make spackage</code></p>

<p>For uploading the package to ppa, First change the <strong>PPA_PGP_KEY</strong> in <a href="../cmake/package_details.cmake#L26">package_details.cmake</a> to details to the full-name of the PGP key  details registered with your ppa account For more details on setting up the pgp key see the <a href="./setting_up_ppa.md">tutorial</a>.Then create a source package by building the target <em>spackage</em>.Once the Source package is build successfully, upload it to your ppa by:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">cd Debian/
dput ppa:&lt;lp-username&gt;/&lt;ppa-name&gt; &lt;packet-&gt;source.changes
</code></pre></div>
<p>building of source package can be tested with:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">cd Debian/robocomp-&lt;version&gt;
debuild -i -us -uc -S
</code></pre></div>
<p>If you are uploading a new version of robocomp, change the version number  accordingly in the <a href="../CMakeLists.txt#L31">toplevel cmake</a> before building, and then upload the source package as mentioned.</p>

<h3>Note:</h3>

<p>If you want to upload another source package to ppa which doesn&#39;t have any changes in the source but maybe in the debian files. you can build the spackage after commenting out <code>set(DEB_SOURCE_CHANGES &quot;CHANGED&quot; CACHE STRING &quot;source changed since last upload&quot;)</code> in <a href="../cmake/package_details.cmake#L27">package_details.cmake</a> so that the the script will only increase the ppa version number and won&#39;t include the source package for uploading to ppa (which otherwise will give an error).</p>

<h2>Installing robocomp from ppa</h2>

<p>First you will need to add the ppa in your sources, and then install robocomp package.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">sudo add-apt-repository ppa:&lt;lp-username&gt;/robocomp
sudo apt-get update
sudo apt-get install robocomp
</code></pre></div>
<p>this will install robocomp along with basic components into /opt/robocomp.</p>

<hr>

<p>Nithin Murali</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/07/02/kripa1/">
        <i>GSoC,</i> Computer vision components and libraries management <p>#1</p>
      </a>
    </h1>

    <span class="post-date">02 Jul 2015</span>

    <p><strong>About me</strong>:Hello, I am Kripasindhu Sarkar, a new PhD student at German Research Center for Artificial Intelligence (DFKI), Kaiserslautern working in the topic of Object Detection in simple and depth images. 
I am extremely interested in the topic of object detection and computer vision; specifically in solving the problem by using theories from human cognition and perception to simulate human way of visualizing the problem. 
But for now, I am focused on getting a very good grasp at the existing engineering (mostly) techniques in the field of computer vision and object detection. 
Before joining here as a PhD student I worked as a Software Engineer at Paypal for 2 years and, prior to that I did my masters and graduation from Indian Institute of Technology Kharagpur (IIT Kharagpur).</p>

<h2>Computer vision components and libraries management</h2>

<p>The project is about designing and implementing a system for object detection and recognition in 3D point clouds and 2D images, and come up with a structured library with a good and easy-to-use APIs.
There has been a good amount of research in this direction and my work was to cherry-pick important ideas and present them as usable components. I&#39;ll now explain in details the various methods I chose to
use as a part of this project.</p>

<h3>Local feature based on 2D images</h3>

<p>The idea is the find local features (like SIFT/SURF/ORB etc) in images of the object to be detected and the given test image. If enough matches are found between the descriptors of the to images an object is defined to be found. Important assumption is that the object to be detected must have textures. Advantage is that we get the complete 6 DOF of the object which might be useful for grasping. This comes in several flavors. </p>

<ol>
<li><p>Planner objects: If we know the object is planner, we can directly compute its tomography (pose) after the match.</p></li>
<li><p>Random objects: If the object is of arbitrary shape it is quite difficult to detect an object with its pose but can be done in a tricky offline phase [1]. A 3D reconstruction is performed through bundle adjustments with the object to be detected to find the 2D - 3D correspondences. On the run time, given an input image, If enough matches are found, the object is detected with its full pose by solving PnP problem. </p></li>
</ol>

<h3>Dense feature based on 2D images:</h3>

<p>The idea is the find features over a grid or a region of an image encoding the properties of that region and use that feature in some classification algorithm to perform detection. Naturally, we need to calculate dense feature over all possible region size over the image and apply the classifier; and thus it is bit slow as well. Also object pose is not identified in this type. Few of them are:</p>

<ol>
<li><p>HOG based simple classification (well known).
Difficulty in implementation: Moderate; HOG implementation with multiscale detector is present in OpenCV; but the training has to be performed separately using 3rd party tool like libsvm/matlab etc (but is straightfwd).</p></li>
<li><p>HOG based Part Based Model: This is the famous and legendary and state of art (not anymore) object detector which uses LSVM.
Difficulty in implementation: Difficult; OpenCV has the detection code, but not that good. Training LSVM is not straight fwd and we need to use the original Matlab implementation of the authors.</p></li>
<li><p>Wevlet based face detector with adaboost: This is also well known face detector algorithm used widely.
Difficulty in implementation: Easy; though the concept is not that straight fwd, it is readily avilable in OpenCV.</p></li>
</ol>

<h3>Detection/Recognition on Depth Images</h3>

<p>If we can get the Point Cloud with some laser scan or Kinect, there are plenty of algorithms to detect object with its pose. Again we have local feature based and global feature based algorithms described below:</p>

<ol>
<li><p>Direct object with local and global features [4]:
Very similar to that of RGB image based algorithm with difference in the types of features. Local features have the advantage that preprocessing steps like segmentation is not required but tends to be slow. On the other hand we need to do segmentation to apply global features in the clusters. But once the segmentation (like identifying planes, etc and different clusters) of the scene is done, we can use the results subsequently. 
Difficulty in implementation: Easy; components of pipeline is available in PCL.</p></li>
<li><p>Object matching using classifiers: 
Global features readily available in PCL and found it to have similar results to a current benchmark but faster (10 seconds for classification testing in the benchmark [2] which uses sliding window based classification on all scales using HOG like descriptors). </p></li>
</ol>

<h2>The library - Open Detection</h2>

<p>It was decided later to have an independent library for Object Detection instead of integrating everything to Robocomp. The result is the inception of a separate library &#39;Open Detection&#39;.
The details of the design of the library is discussed in the next blog.</p>

<hr>

<p>[1] I. Gordon and D. G. Lowe, “What and where: 3d object recognition with accurate pose,” in Toward Category-Level Object Recognition, ser. Lecture Notes in Computer Science, J. Ponce, M. Hebert, C. Schmid, and A. Zisserman, Eds., vol. 4170. Springer, 2006, pp. 67–82.
[2] MOPED: A Scalable and Low Latency Object Recognition and Pose Estimation System
[3] Object Detection with Discriminatively Trained Part Based Models
[4] Aldoma, A.; Marton, Zoltan-Csaba; Tombari, F.; Wohlkinger, W.; Potthast, C.; Zeisl, B.; Rusu, R.B.; Gedikli, S.; Vincze, M., &quot;Tutorial: Point Cloud Library: Three-Dimensional Object Recognition and 6 DOF Pose Estimation,&quot; Robotics &amp; Automation Magazine, IEEE , vol.19, no.3,</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/website/page3">Older</a>
  
  
    
      <a class="pagination-item newer" href="/website">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
