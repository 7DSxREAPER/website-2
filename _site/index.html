<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      RoboComp &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/website/public/css/poole.css">
  <link rel="stylesheet" href="/website/public/css/syntax.css">
  <link rel="stylesheet" href="/website/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/websitepublic/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/websitepublic/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/website">
          RoboComp
        </a>
      </h1>
      <p class="lead">A simple robotics framework.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/website">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/Blog/">Blog</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC15/">GSoC'15</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC16/">GSoC'16</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/projects/">Projects</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/about/">About</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/website/contact/">Contact</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/install/">Install</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
      <a class="sidebar-nav-item" href="http://robocomp.readthedocs.org">Tutorials</a>
      <a class="sidebar-nav-item" href="https://github.com/robocomp">GitHub project</a>
      
      <span class="sidebar-nav-item">Currently v1.0.0</span>
    </nav>

    <p>&copy; 2016. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <i><b>RoboComp</b> is an open-source Robotics framework providing the tools to create and modify software components that communicate through public interfaces. Components may require, subscribe, implement or publish interfaces in a seamless way. Building new components is done using two domain specific languages, IDSL and CDSL. With IDSL you define an interface and with CDSL you specify how the component will communicate with the world. With this information, a code generator creates C++ and/or Python sources, based on CMake, that compile and execute flawlessly. When some of these features have to be changed, the component can be easily regenerated and all the user specific code is preserved thanks to a simple inheritance mechanism.</i>


<hr>

<div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/21/nithin11/">
        Packaging FCL and libccd
      </a>
    </h1>

    <span class="post-date">21 Aug 2015</span>

    <p>I assume that you have an debian package folder in your source directory. If not refer to the  intro to debian packaging tutorial.</p>

<p>Now lets assume that you are going to upload the package for the first time into a ppa.</p>

<h3>Creating source package</h3>

<ul>
<li>Rename  the source directory into <project_name>-<version> eg libccd-2.01</li>
<li>Crate a .tar.gz compressed file of the source directory, and place it outside source directory</li>
<li>Rename the compressed file into <project_name>_<version>.orig.tar.gz</li>
<li>Now run <code>debuild  -k&lt;gpg_key&gt; -S -sa</code> if you want to include the whole source tar in upload</li>
<li>Or run <code>debuild  -k&lt;gpg_key&gt; -S -sd</code> if you don&#39;t want to include the whole source tar in upload</li>
</ul>

<h3>So when should you upload the source?</h3>

<p>when you are uploading for the first time (obviously) and whenever you make some changes to your source code. but as launchpad wont allow files with same name, you should increase your version number so that the source tar get a new name. For increasing the version number you should increase it in changelog for example in this case <code>fcl (1.0-0ppa0) vivid; urgency=low</code> increase 1.0-0ppa0 to 1.1-0ppa0 also remember to rename all names accordingly (source folder and source tar)</p>

<p>but if you have only changed some file in the debian directory. For example, edited changelog or added a dependency. In those cases you can skip the source upload but in such cases you have to increase your ppa number in your changelog  for example in this line  <code>fcl (1.0-0ppa0) vivid; urgency=low</code> change 0ppa0 to 0ppa1.</p>

<h3>uploading</h3>

<p>Now once you have generated the .source_changes file use dput to upload </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">dput ppa:&lt;your-lp-id&gt;/&lt;name&gt; &lt;file_name&gt;.source_changes
</code></pre></div>
<h3>NB</h3>

<ul>
<li>if you want to add any dependency, edit the file debian/control add add it to <code>Depends</code> or <code>Build-Depends</code> field</li>
<li>if you want to change the target generation , edit the distribution name in first line of debian/changelog</li>
</ul>

<hr>

<p>Nithin Murali</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/20/rajath3/">
        <i>GSoC,</i> After Midterms
      </a>
    </h1>

    <span class="post-date">20 Aug 2015</span>

    <p>At the start of the second term, I finished developing the version-2 of the website.</p>

<p>Version-2 : https://github.com/robocomp/website/tree/version-2</p>

<p>After learning from the previous-2 version built the Version-3(Current) of the website using Jekyll.</p>

<p>Version-3 : https://github.com/robocomp/website/tree/gh-pages</p>

<p>Website : www.robocomp.net</p>

<p>Parallely I started developing simple components for robocomp which would introduce new users to the framework. I have implemented the components in both the languages which robocomp supports - c++ and python. Also have documented the same.</p>

<p>C++ components : https://github.com/rajathkumarmp/RoboComp-Components</p>

<p>Python components : https://github.com/rajathkumarmp/RoboComp-Python-Components</p>

<p>Documentation : https://github.com/rajathkumarmp/RoboComp-Docs</p>

<p>Future : I will be writing starter components for each of the available interface so that a new user can easily get started. After having executed most of the already available components in the robocomp organization, In the coming days I will be working on components using PCL and explore other possibilities with the framework. It has been a great learning experience so far and I am hungry for more. Cheers!</p>

<hr>

<p>Rajath Kumar M.P</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/20/mercedes6/">
        <i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#6</p> grasping object
      </a>
    </h1>

    <span class="post-date">20 Aug 2015</span>

    <p><strong>Grasping object</strong> : This post will describe the planning system that implements Robocomp in order to provide to the robot a full functionality. In order for a robot be able to carry out a mission as &quot;take the cup from the table and take it to the kitchen,&quot; it needs something that in robotics calls <code>planner</code>. In this post we move away the issue of inverse kinematics and we dive into the field of artificial intelligence, making a slight revision of existing planners and delving into the planner using robocomp.</p>

<h3>Planning</h3>

<p>As the name suggests, the planning is to generate plans. Plans to be executed by a robot in order to reach a goal. These objectives are often complex and require the execution of a series of steps that must be organized in the best possible manner to achieve the objective with an effort and within a reasonable time.</p>

<p>In order for a planner can build a series of plans, it needs an initial state of the world, a desired end state (or several) and a set of rules. For example, take the objective of frying an egg. We start with an initial world consisting of a kitchen with the necessary elements: an egg with a dozen eggs, a can of oil, a frying pan, a slotted spoon, a plate and a stove. And we have a set of rules, like <code>to pour oil</code>, <code>to water plants</code>, <code>to crack egg</code>, <code>to stir</code>, <code>to serve</code>, <code>to light fire</code> and <code>to put the fire out</code>. The duty of the planner is to select and order those rules that allow us to go from the initial state of the world (with raw eggs) to the final state in which a fried egg is served  on a plate. Thus, the planner would eliminate the rule of &quot;to water plants&quot;, and would order the rest of rules as follows:</p>

<ol>
<li><code>to light fire</code>: in order to light the stove.</li>
<li><code>to pour oil</code>: to pour oil into the pan</li>
<li><code>to crack egg</code>: to crack the egg and throw it into the pan</li>
<li><code>to stir</code>: to catch the slotted spoon and to go stirring the oil for frying the egg well.</li>
<li><code>to put the fire out</code>: when the egg is fried the stove is turned off.</li>
<li><code>to serve</code>: to serve the fried egg on the plate</li>
</ol>

<p>With these steps (very simplified) we get that our robot prepares us a fried egg... Although the example is miserable, we can see that any action we do and that we find it easy to execute, for a robot is quite complicated. That is why planning is a very complex field of artificial intelligence.</p>

<h3>Planning Domain Definition Language (PDDL)</h3>

<p>This section will introduce the reader slightly in the planning language more used in artificial intelligence: PDDL. It was created in 1998 by Drew McDermott and his team for use in that year&#39;s edition of International Planning Competition. The aim of PDDL is to standardize the planning languages for greater reuse of planning domains. Its operation is relatively simple:</p>

<p>Take for example, the following domain to create geometric shapes: We presume that in our initial world there is always a vertex node. The rules increase the number of vertices nodes to create geometric identities. For example, the <code>segment</code> rule creates from the initial node a line segment, the rule <code>triangle</code> creates from the line segment an equilateral triangle, the rule &quot;square&quot; creates from the equilateral triangle a square... and so on until an octagon. In the PDDL file, the rules would be:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">(define (domain AGGL)
(:predicates
            (firstunknown ?u)
            (unknownorder ?ua ?ub)

            (isA ?n) # IS A NODE?
            (isB ?n)
            (isC ?n)
            (isD ?n)

            (union ?u ?v) # TWO NODES ?u AND ?v are united?
    )

    (:functions
            (total-cost)
    )

    (:action segment
            :parameters ( ?va ?vListAGMInternal ?vlist0 )
            :precondition (and (isA ?va) (firstunknown ?vlist0) (unknownorder ?vlist0 ?vListAGMInternal) (not(= ?vListAGMInternal ?vlist0)) )
            :effect (and (not (firstunknown ?vlist0)) (not (unknownorder ?vlist0 ?vListAGMInternal)) (firstunknown ?vListAGMInternal) (isB ?vlist0) (union ?va ?vlist0) (increase (total-cost) 1)
            )
    )

    (:action triangle
            :parameters ( ?va ?vb ?vListAGMInternal ?vlist0 )
            :precondition (and (isA ?va) (isB ?vb) (firstunknown ?vlist0) (unknownorder ?vlist0 ?vListAGMInternal) (not(= ?vListAGMInternal ?vlist0)) (union ?va ?vb) )
            :effect (and (not (firstunknown ?vlist0)) (not (unknownorder ?vlist0 ?vListAGMInternal)) (firstunknown ?vListAGMInternal) (isC ?vlist0) (union ?vb ?vlist0) (union ?vlist0 ?va) (increase (total-cost) 1)
            )
    )

    (:action square
            :parameters ( ?va ?vc ?vb ?vListAGMInternal ?vlist0 )
            :precondition (and (isA ?va) (isC ?vc) (isB ?vb) (firstunknown ?vlist0) (unknownorder ?vlist0 ?vListAGMInternal) (not(= ?vListAGMInternal ?vlist0)) (union ?va ?vb) (union ?vb ?vc) (union ?vc ?va) )
            :effect (and (not (firstunknown ?vlist0)) (not (unknownorder ?vlist0 ?vListAGMInternal)) (firstunknown ?vListAGMInternal) (isD ?vlist0) (union ?vc ?vlist0) (union ?vlist0 ?va) (not (union ?vc ?va)) (increase (total-cost) 1)
            )
    )
  )
</code></pre></div>
<p>To represent the initial world from which we start and the final world that we want to go, we should implement a file like this: </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">(define (problem myProblemPDDL)

    (:domain AGGL ) # planning domain ---&gt; set of rules
    (:objects
            A_1
            unknown_0
            unknown_1
            unknown_2
            unknown_3
    )

    (:init
            (= (total-cost) 0)
            (firstunknown unknown_0)
            (unknownorder unknown_0 unknown_1)
            (unknownorder unknown_1 unknown_2)
            (unknownorder unknown_2 unknown_3)
            (isA A_1) # ----&gt;  there is a initial vertex node
    )

    (:goal
            (exists ( ?A_1001 ?B_1002 ?C_1003 ?D_1004 ) # -----&gt; we want a final world with a square
                    (and
                            (isA ?A_1001)
                            (isB ?B_1002)
                            (isC ?C_1003)
                            (isD ?D_1004)
                            (unido ?A_1001 ?B_1002)
                            (unido ?B_1002 ?C_1003)
                            (unido ?C_1003 ?D_1004)
                            (unido ?D_1004 ?A_1001)
                    )
            )
    )
    (:metric minimize (total-cost))
  )
</code></pre></div>
<p>This language proves to be relatively intuitive, easy to develop and test. However, in Robocomp we opted to do our own planning language: AGM.</p>

<h3>Active Grammar-based Modeling</h3>

<p>AGM is the result of Luis Manso&#39;s PhD thesis that &quot;dealt with making software systems for robots more scalable, flexible and easier to develop using software engineering for robotics [...] and enhancing active perception in robots using a grammar-based technique named active grammar-based modeling and a specially tailored novelty-detection algorithm named cognitive subtraction&quot;<a href="http://ljmanso.com/thesis.php">1</a>. To not extend much this post, you can check the working of the AGM on its <a href="http://ljmanso.com/agm/">official website</a>. We will target only that AGM proves to be a useful graphical tool to program rules and  to test domains and problems, and its grammar has a reminiscent of the PDDL language (has a AGM to PDDL converter). Let&#39;s look at the same example as before but now in AGM:</p>

<p>This would be the set of rules in a .aggl file:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">// START OF THE FILE:
segment : active(1)
{
    {
            a:A(-175,-15)
    }
    =&gt;
    {
            a:A(-415,-5)
            b:B(-150,-5)
            a-&gt;b(union)
    }
}

triangle : active(1)
{
    {
            a:A(-330,-10)
            b:B(-70,-15)
            a-&gt;b(union)
    }
    =&gt;
    {
            a:A(-440,170)
            c:C(-270,10)
            b:B(-70,170)
            a-&gt;b(union)
            b-&gt;c(union)
            c-&gt;a(union)
    }
}

square : active(1)
{
    {
            a:A(-385,80)
            c:C(-245,-125)
            b:B(-105,80)
            a-&gt;b(union)
            b-&gt;c(union)
            c-&gt;a(union)
    }
    =&gt;
    {
            a:A(-460,125)
            c:C(-155,-45)
            b:B(-150,125)
            d:D(-460,-40)
            a-&gt;b(union)
            b-&gt;c(union)
            c-&gt;d(union)
            d-&gt;a(union)
    }
}
</code></pre></div>
<p>The initial world model is stored in a xml file:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">&lt;AGMModel&gt;
    &lt;symbol id=&quot;1&quot; type=&quot;A&quot; /&gt;  #There is only a vertex node (symbol) with the identifier &quot;1&quot; and te type &quot;A&quot;
&lt;/AGMModel&gt;
</code></pre></div>
<p>The goal or target world is stored in another xml file:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">&lt;AGMModel&gt;
    # NODES:
    &lt;symbol id=&quot;a&quot; type=&quot;A&quot; /&gt;
    &lt;symbol id=&quot;b&quot; type=&quot;B&quot; /&gt;
    &lt;symbol id=&quot;c&quot; type=&quot;C&quot; /&gt;
    &lt;symbol id=&quot;d&quot; type=&quot;D&quot; /&gt;
    # LINKS BETWEEN NODES (RELATIONSHIPS)
    &lt;link src=&quot;a&quot; dst=&quot;b&quot; label=&quot;union&quot; /&gt;
    &lt;link src=&quot;b&quot; dst=&quot;c&quot; label=&quot;union&quot; /&gt;
    &lt;link src=&quot;c&quot; dst=&quot;d&quot; label=&quot;union&quot; /&gt;
    &lt;link src=&quot;d&quot; dst=&quot;a&quot; label=&quot;union&quot; /&gt;
&lt;/AGMModel&gt;
</code></pre></div>
<h3>Component architecture</h3>

<p>Explained more or less the planners, we will explain the architecture of components developed by robocomp for the robot to be able to carry out actions. Oversimplifying the question in order to that the reader make a clear idea of how the architecture works, we can say that this architecture is divided into three levels:</p>

<p><img src="https://github.com/mercedes92/VisualIKExperiment/blob/master/images/Arquitectura.png?raw=true" alt="Alt text"></p>

<ol>
<li>First we need a problem domain (a set of rules), like problems in a home environment and a representation of the environment (a model of the robot and its environment).</li>
<li>In the top we have the AGM planner. Given an initial world and an objective world, it is in charge of generating a plan to reach the goal with the domain defined.</li>
<li>In the second label, we have a special component, the <code>executive</code>. Basically he is responsible for transmitting the plan generated by the planner to the lower components. These components perform their actions and alter the representation of the environment. so the executive will have to call the scheduler to verify that these changes on the environment are correct and possible. If the changes are verified, the executive will update the representation.</li>
<li>In the third label we have the agent components. These are the components that receive the orders of the executive. They uses the operations of the lower component to change the representation. When they finish their execution, they publish the changes in order to be analyzed by the executive.</li>
<li>In the botton we have the basic components. They are those who perform basic calculations. For example, our <code>inversekinematic</code> component, our <code>visualik</code> component and our <code>ikGraphGenerator</code> component are in this level.</li>
</ol>

<p>So the challenge that we have to complete is to create an agent that uses our three inverse kinematics components in order to reach a goal: that the robot take a cup. This component is called <code>graspingAgent</code> and is currently being developed by the laboratory. I would like to delve into its operation, but we still needs to implement many things and GSoC is coming to an end :(</p>

<p>It&#39;s a shame to have to say goodbye with the work so close to being finished. But it doesn&#39;t matters, the next year the robot will be serving coffee to Robolab guys XD.</p>

<p>Bye!</p>

<p><img src="http://photos.gograph.com/thumbs/CSP/CSP705/k24410287.jpg" alt="Alt text"></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/19/kripasindhu_sarkar_blog_4/">
        GSoC, Computer vision components and libraries management -Open Detection <p>#4</p>
      </a>
    </h1>

    <span class="post-date">19 Aug 2015</span>

    <p><strong>Experience</strong> 
It has been a quite a ride through this Google Summer of Code. Writing a completely new library with complete building framework/rest framework for documentation or tutorial/and Doxygen framework for auto documentation-class diagrams was challenging. It was equally exciting as well. Because of this I believe that I have acquired quite a good &#39;library maintainer/admin&#39; skills. I will continue contributing into this library after the finish of this GSoC.</p>

<p>After the challenges in building library support tools, the next major challenge was library design. Since, there was no previous &#39;coding flavor/design&#39; I had to come up with the polymorphic and repeatable class design. There was so much confusion in choosing one alternative among the so many choices of good design. While the progress of the library I had to redesign the framework, and and remove structures, include namespaces and several things. I initially had thought that the design will be pretty much stable after the first month, which was not true. I still feel that the main design may need to be changed in the future to have a more logical structure. I hope this constant effort of keeping a good design will be helpful for the library users, and in the end, won&#39;t end up with an unstructured library like OpenCV. In the future I&#39;ll always keep an eye open for the design in general. </p>

<p>Now coming to the actual work of implementing different algorithm, I had really good experience in getting my hands dirty with variety of popular object detection methods. This was exactly what I had thought from this project. Taking out three months and working on the popular methods of your research topic is probably important and I hope that I&#39;ll make a good us of this in the future.</p>

<p><strong>Learning</strong> 
Here I&#39;ll point out some of the tools and resources I have used while building the library.</p>

<h4>CMake building framework:</h4>

<p>I read and used ideas from the CMakeLists files of <strong>VTK, pcl</strong> and <strong>OpenCV</strong> (not that much); and chose stuffs among them depending what I thought was interesting and also modified them according to my needs. But in general, I used PCL cmake framework as my major reference.</p>

<h4>Library design:</h4>

<p>I have always found the design of <strong>VTK</strong>, beautiful; so chose it to get the polymorphic design of the classes. Even though fully templated policy based design is faster than polymorphic classes, I chose the polymorphic one as it gives way more flexibility without cluttering the design (ideas taken from vtk). I still am not sure if the performance loss through this design will affect in the future. </p>

<h4>Algorithms:</h4>

<p>I had a good background in PCL, so Point Cloud based detection was not difficult and used their wonderful 3d<em>recognition</em>framework in my backend. Detection based on 2D features is my own contribution and other 2D global detectors (like HOG) was pretty straightforward from OpenCV. 
One good learning was the implementation of HOG Trainer which I worked on few open source code and modified them according to my own needs (like hard negative training etc).</p>

<h2>Future</h2>

<p>This project does not ends with the end of GSoC. I plan to do the following in the short term future:</p>

<ul>
<li>Host a website as a homepage and links to documentation/tutorials etc.</li>
<li>Identify how is the website automatically updated with a push in previous libraries like PCL etc. They probably use some demon which runs everyday. Learn and implement it.</li>
<li>Improve the documentations, write some descriptive tutorials. </li>
<li>Properly launch the library, with its website; first among the colleagues of our lab; then in public blogs.</li>
</ul>

<hr>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/19/kripasindhu_sarkar_blog_3/">
        GSoC, Computer vision components and libraries management - Open Detction <p>#3</p>
      </a>
    </h1>

    <span class="post-date">19 Aug 2015</span>

    <p><strong>Contributions after Mid-term</strong>
Following are the contributions towards the project <em>after</em> the mid-term evaluations:</p>

<ul>
<li><p>HOG based detection (with demo).</p></li>
<li><p>Face-recognition: training as well as recognition (with demo).</p></li>
<li><p>Cascade based detection (with demo).</p></li>
<li><p>Functionality for confidence in a detection (to handle uncertainty).</p></li>
</ul>

<p>That concluded the functionality of whatever planned for this summer of code. Other than the planned work, I included some other new functionality which came in between and I thought was required. The other additions are: </p>

<ul>
<li><p>Addition of two types of detection functions for every detection class. </p>

<p>a. <em>detectOmni</em> : This is the function whose purpose is to detect an object in an entire scene. Thus, other than the type of detection we also have information about the <em>location</em> of the detection w.r.t. the scene. This was the only function previously implemented.</p>

<p>b. <em>detect</em> : The purpose of this function is to perform detection on a segmented scene or an &#39;object candidate&#39;. i.e. the entire scene is considered as an &#39;object&#39; or an detection. This function was an extra addition for most of the classes. For global descriptor based classes like ODHOGDetector this function came very naturally.</p></li>
<li><p><strong>Standardized the training database</strong>:. Each detection class identifies its own database (which is a unique folder as of now). Training data of all classes are now put together under a single &#39;trained_data&#39; directory and the rest is resolved by the class automatically.</p></li>
<li><p>The class <strong>ODDetectorMultiAlgo</strong>: This is one of the most interesting class and interesting contribution in this project. The idea is to run multiple detection algorithm on a same segmented or unsegmented scene (i.e. run both <em>detect</em> and <em>detectOmni</em> function) and provide the result. That is, using this class one can do object detection/recognition using multiple algorithms and provide outcome of detections (eg. people detected by HOG, face detected by Cascade, bottle detected PnPRansac in a same image). Because of the nice polymorphic design of the detection classes, one can use the concept easily with any number of detection classes with their own parameters, but with <em>ODDetectorMultiAlgo</em> one can only take benefit of the default parameters of the included classes.</p></li>
<li><p>HOG training: There are no standard training module for training HOG (in OpenCV or in other standard library) and people are mostly confused in training svm with the HOG detector available from OpenCV. This formed a motivation for completing the HOG pipeline and provide a training module. The crucial contribution includes <em>hard negative</em> training mode where the training is repeated with hard examples or false positive windows as negative windows. I hope that this will be really a helpful contribution for the computer vision community. </p></li>
</ul>

<h2>Design changes, learning resources, and overall learning experiences:</h2>

<p>In the next blog I&#39;ll add the different sources I used to design and implement the above tasks and the things I learnt in this process. </p>

<hr>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/website/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>
    </div>

  </body>
</html>
